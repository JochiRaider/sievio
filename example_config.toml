# example_config.toml
# Comprehensive example of the TOML layout supported by SievioConfig.
# All tunable fields are shown with their library defaults; tweak as needed.
# Array-of-table entries under [[sources.specs]] and [[sinks.specs]] hold the
# declarative wiring for sources/sinks; everything else sets global defaults.
# Use [sources.defaults.<kind>] and [sinks.defaults.<kind>] to share per-kind
# overrides across multiple specs (matching the SourceSpec/SinkSpec.kind values).

# ---------------------------------------------------------------------------
# Source defaults (apply to specs unless overridden per-spec)
# ---------------------------------------------------------------------------
[sources.local]
# include_exts = [".py", ".md"] # Optional: only include these extensions (omit to accept all)
# exclude_exts = [".png"]       # Optional: drop these extensions
skip_hidden = true            # Skip dotfiles / dot-directories
follow_symlinks = false       # Control symlink traversal
respect_gitignore = true      # Honor .gitignore when scanning local_dir
max_file_bytes = 209715200    # 200 MiB cap for local reads
# read_prefix_bytes = 4096      # Optional: peek prefix for very large files
read_prefix_for_large_files_only = true

[sources.github]
per_file_cap = 209715200      # Hard cap (bytes) for each file inside the GitHub zipball
max_total_uncompressed = 2147483648  # Global uncompressed size cap (bytes) across all zip members
max_members = 200000          # Maximum number of files allowed in the zipball before aborting
max_compression_ratio = 100.0 # Zip bomb guard: maximum allowed file_size / compressed_size ratio
# include_exts = [".rst"]      # Optional extension allowlist (only ingest these from the zipball)
# exclude_exts = [".png"]      # Optional extension denylist (never ingest these from the zipball)

[sources.pdf]
timeout = 60                  # Per-request timeout for web PDF downloads (seconds)
max_pdf_bytes = 209715200     # Maximum bytes to download per PDF; abort when exceeded
max_links = 200               # Maximum discovered links to chase from a scraped page
require_pdf = true            # Drop responses that do not sniff as PDFs when true
include_ambiguous = false     # Allow links without an explicit .pdf suffix when true
retries = 1                   # Number of retry attempts per PDF URL
user_agent = "sievio/0.1 (+https://github.com/jochiraider/sievio)" # User-Agent string for web PDF requests
# client is injected programmatically if you want to reuse a SafeHttpClient
download_max_workers = 4      # Number of concurrent PDF downloads (0/negative → auto based on URL count)
# download_submit_window = 16   # Optional cap on in-flight download tasks (else max_workers * 4)
download_executor_kind = "thread"  # Executor for downloads ("thread" only; "process" falls back to thread)

[sources.csv]
default_text_column = "text"  # Column name to treat as the source text when using csv_text source specs
# default_delimiter = ","       # Default delimiter; inferred from extension when omitted (.tsv → tab)
encoding = "utf-8"            # Default file encoding for CSV/TSV sources

[sources.sqlite]
default_text_columns = ["text"]  # Default columns to concatenate into text when using sqlite source specs
batch_size = 1000                # Number of rows to fetch per batch from SQLite
# download_max_bytes = 104857600   # Optional cap on bytes when downloading remote SQLite DBs
retries = 2                      # Number of retry attempts for downloading a remote SQLite DB

# ---------------------------------------------------------------------------
# Optional per-kind defaults (applied to specs before per-spec overrides)
# Keys match SourceSpec.kind (e.g., "local_dir", "github_zip") or fallback names
# like "pdf"/"sqlite" used by some factories.
# ---------------------------------------------------------------------------
[sources.defaults.local_dir]
# max_file_bytes = 104857600     # Example: tighten the default for all local_dir specs
# respect_gitignore = false

[sources.defaults.github_zip]
# include_exts = [".md", ".rst"]
# exclude_exts = [".png"]

[sources.defaults.web_pdf_list]
# download_max_workers = 8
# include_exts = [".pdf"]

[sources.defaults.web_page_pdf]
# max_links = 100
# include_exts = [".pdf"]

[sources.defaults.csv_text]
# default_delimiter = ","
# encoding = "utf-8"

[sources.defaults.sqlite]
# batch_size = 2000
# download_max_bytes = 104857600

# ---------------------------------------------------------------------------
# Declarative sources (each spec uses the defaults above unless overridden)
# Uncomment the blocks you need.
# ---------------------------------------------------------------------------
[[sources.specs]]
kind = "local_dir"
options = { root_dir = "path/to/repo" }  # Walk a local repository directory tree

[[sources.specs]]
kind = "github_zip"
options = { url = "https://github.com/owner/repo/archive/refs/heads/main.zip" }  # Download and scan a GitHub zipball

[[sources.specs]]
kind = "web_pdf_list"
options = { urls = ["https://example.com/a.pdf", "https://example.com/b.pdf"], add_prefix = "webpdfs" } # Direct list of PDF URLs

[[sources.specs]]
kind = "web_page_pdf"
options = { page_url = "https://example.com/page-with-links", add_prefix = "pagepdfs" } # Scrape a page for PDF links

[[sources.specs]]
kind = "csv_text"
[sources.specs.options]
paths = ["data/train.csv", "data/extra.tsv.gz"]  # One or more CSV/TSV files (optionally gzipped)
text_column = "text"         # Name of the text column (defaults to sources.csv.default_text_column)
delimiter = ","              # Column delimiter (defaults to sources.csv.default_delimiter)
encoding = "utf-8"           # Encoding for these CSV/TSV files
has_header = true            # Whether the first row is a header row
text_column_index = 0        # Index of text column when has_header = false

[[sources.specs]]
kind = "sqlite"
[sources.specs.options]
db_path = "data/example.db"               # Local path where the SQLite DB should reside
db_url = "https://example.com/example.db"  # Optional URL to download the DB if db_path is missing
table = "documents"                       # Table to read from (mutually exclusive with sql)
# sql = "SELECT id, text FROM documents"  # Custom SQL query instead of table-based selection
text_columns = ["text"]                   # Columns whose contents are concatenated into text
id_column = "id"                          # Column to use for stable record IDs / paths
where = "text IS NOT NULL"               # Optional WHERE clause to filter rows
batch_size = 1000                        # Overrides sources.sqlite.batch_size for this spec
download_timeout = 60.0                  # Per-request timeout for downloading the DB (seconds)
download_max_bytes = 104857600           # Maximum allowed size of the downloaded DB (bytes)
retries = 2                              # Retries when downloading the DB fails

# ---------------------------------------------------------------------------
# Decode / chunk / language ID / pipeline behavior
# ---------------------------------------------------------------------------
[decode]
normalize = "NFC"             # Unicode normalization (comment out to disable)
strip_controls = true         # Strip zero-width and unsafe control characters
fix_mojibake = true           # Attempt to repair common UTF-8-as-cp1252 mojibake
# max_bytes_per_file = 1048576  # Optional soft cap on bytes decoded per file (truncate beyond this)

[chunk]
# tokenizer_name = "cl100k_base" # Optional tokenizer name for token counting (requires [tok] extra)
attach_language_metadata = true  # Attach language labels to chunks in record metadata

  [chunk.policy]
  mode = "doc"                # "doc" or "code"
  target_tokens = 1700        # Approximate target token count per chunk
  overlap_tokens = 40         # Approximate token overlap between consecutive chunks
  min_tokens = 400            # Minimum token count before forcing a flush
  semantic_doc = false        # Enable paragraph/sentence-aware splitting for long doc blocks
  # semantic_tokens_per_block = 600  # Target tokens per semantic sub-block when semantic_doc is true

[language]
enabled = true                # Enable human language identification
backend = "baseline"          # "baseline" or "lingua" (requires [langid] extra)

[code_lang]
enabled = true                # Enable code/doc language identification
backend = "baseline"          # "baseline" or "pygments" (requires [langid] extra)
# hints = {}                  # Optional LanguageConfig overrides (code_exts, doc_exts, ext_lang)

[pipeline]
# extractors / bytes_handlers / file_extractor are typically wired in Python.
max_workers = 0               # 0 → auto (os.cpu_count or 1)
# submit_window = 8            # Optional max number of pending tasks (else max_workers * 4)
fail_fast = false             # Abort the pipeline on the first worker error when true
executor_kind = "auto"        # Executor kind: "thread", "process", or "auto" (auto-infers based on sources/handlers)

# ---------------------------------------------------------------------------
# Sinks (defaults + declarative specs)
# ---------------------------------------------------------------------------
[sinks]
# context is typically provided programmatically (RepoContext)
output_dir = "."              # Base directory for outputs when explicit paths are not given
# primary_jsonl_name is filled by builders when default_jsonl_prompt is used
compress_jsonl = false        # Gzip-compress the primary JSONL when true
jsonl_basename = "data"       # Default basename for JSONL when factories derive paths
# primary_jsonl_name = "data.jsonl"  # Builder may set this automatically; override if needed

[sinks.defaults]
# Per-kind defaults keyed by SinkSpec.kind; mostly useful for plugins or to
# avoid repetition across multiple specs.
[sinks.defaults.default_jsonl_prompt]
compress_jsonl = true
# include_prompt_file = true
# heading_fmt = "### {path} [chunk {chunk}]"

[sinks.defaults.parquet_dataset]
# compression = "zstd"
# row_group_size = 100000

[sinks.prompt]
heading_fmt = "### {path} [chunk {chunk}]"  # Template for headings in the prompt text sink
include_prompt_file = true   # Enable or disable writing the prompt text sink

[[sinks.specs]]
kind = "default_jsonl_prompt"
[sinks.specs.options]
jsonl_path = "out/data.jsonl"        # Path to the primary JSONL output
prompt_path = "out/data.prompt.txt"  # Path for the prompt text sink (comment out to skip prompt sink)

[[sinks.specs]]
kind = "parquet_dataset"
[sinks.specs.options]
path = "out/data.parquet"    # Target Parquet file or dataset directory
text_field = "text"          # Field name for chunk text column in Parquet
meta_field = "meta"          # Field name for metadata column in Parquet
partition_by = ["repo"]      # Optional list of metadata keys to partition the dataset by
row_group_size = 100000      # Target row group size for Parquet writes
compression = "snappy"       # Parquet compression codec (e.g., "snappy", "zstd", "gzip")
overwrite = true             # Overwrite an existing Parquet target when true

# ---------------------------------------------------------------------------
# HTTP client defaults
# ---------------------------------------------------------------------------
[http]
timeout = 60.0               # Default timeout (seconds) for HTTP requests
max_redirects = 5            # Maximum number of redirects allowed per request
allowed_redirect_suffixes = ["github.com"]  # Domains allowed for cross-host redirects
# client can be supplied programmatically to reuse a SafeHttpClient
as_global = true             # Install the built client as the global SafeHttpClient when true

# ---------------------------------------------------------------------------
# Quality control (QC)
# ---------------------------------------------------------------------------
[qc]
enabled = false              # Enable quality scoring and gating when true
write_csv = false            # Emit a QC CSV alongside JSONL using QC scorers
csv_suffix = "_quality.csv"  # Suffix to use for QC CSV filenames
write_signals_sidecar = false # Emit QC signals sidecar (CSV/Parquet) when true
signals_suffix = "_signals.csv" # Suffix for QC signals sidecar (ignored for Parquet)
signals_format = "csv"       # "csv" or "parquet" for QC signals sidecars
fail_on_error = false        # Raise on QC errors rather than logging and continuing
min_score = 60.0             # Minimum QC score required to keep records (inline/advisory modes)
drop_near_dups = false       # Drop near-duplicate records based on QC duplicate detection
exact_dedup = true           # Use exact content hashes to short-circuit global dedup (when available)
mode = "inline"               # "inline", "advisory", "post", or "off"
parallel_post = false        # Use process-based parallelism for post-QC when true
# post_executor_kind = "process" # Executor kind override for post-QC scoring
# post_max_workers = 4        # Maximum workers for post-QC (defaults to pipeline.max_workers)
# post_submit_window = 16     # Submit window for post-QC executor (defaults to pipeline.submit_window)
# scorer_id selects a scorer (None = registry default); scorer_options passes scorer-specific settings

[qc.scorer_options]
lm_model_id = false            # Optional LM id for perplexity (requires QC extras)
device = "cuda"               # Device for LM-based perplexity
dtype = "bfloat16"            # Torch dtype string for LM
local_files_only = false      # Restrict model loading to local files
simhash_hamm_thresh = 4       # Hamming threshold for SimHash near-dup detection
enable_minhash = true         # Enable MinHash-based duplicate detection
minhash_perms = 128           # Number of permutations for MinHash signatures
minhash_bands = 32            # Number of bands for MinHash LSH
minhash_shingle_k = 5         # Byte-level shingle size for MinHash
minhash_jaccard_thresh = 0.82 # Jaccard threshold to treat MinHash matches as near-duplicates
enable_gopher = true          # Enable Gopher-style quality heuristic
gopher_weight = 0.10          # Weight applied to the Gopher quality score

[qc.scorer_options.heuristics]
target_code_min = 2000       # Lower bound of target token band for code-like content
target_code_max = 4000       # Upper bound of target token band for code-like content
target_log_min = 1000        # Lower bound of target token band for log/structured content
target_log_max = 2000        # Upper bound of target token band for log/structured content
target_text_min = 1500       # Lower bound of target token band for text/doc content
target_text_max = 2000       # Upper bound of target token band for text/doc content
target_other_min = 1000      # Lower bound of target token band for other content
target_other_max = 3000      # Upper bound of target token band for other content
repetition_k = 16            # k-gram length for repetition/near-dup heuristics
code_short_line_threshold = 60  # Maximum line length to be considered a "short" code line
code_punct_weight = 0.5      # Weight of punctuation density in code complexity score
code_short_line_weight = 0.5 # Weight of short-line ratio in code complexity score
simhash_window = 128         # Window size for simhash-based near-duplicate detection
# simhash_hamm_thresh = 3     # Maximum Hamming distance to treat two hashes as near-duplicates
# enable_minhash = true       # Enable MinHash-based global duplicate detection
# minhash_perms = 128         # Number of permutations for MinHash signatures
# minhash_bands = 32          # Number of bands for MinHash LSH
# minhash_shingle_k = 5       # Byte-level shingle size for MinHash
# minhash_jaccard_thresh = 0.8 # Jaccard threshold to treat MinHash matches as near-duplicates

[qc.scorer_options.global_dedup]
# Optional SQLite-backed global MinHash dedup store (core/dedup_store.py); leave unset to disable.
# path = "out/global_dedup.db"
# read_only = false            # true to reuse a pre-seeded DB without mutating it

[qc.safety]
enabled = false              # Enable safety/PII scoring
mode = "inline"              # "inline", "advisory", "post" (unsupported), or "off"
annotate_only = false        # When true, never drop on safety even in inline mode
fail_on_error = false        # Raise on safety scorer errors instead of logging
scorer_id = "default_safety" # Safety scorer to use (None → registry default)
# allowed_licenses = ["mit", "apache-2.0"] # Optional license allowlist
# toxicity_threshold = 0.5     # Optional toxicity cutoff for default safety scorer
# scorer_options = {}          # Extra options passed to the safety scorer

# ---------------------------------------------------------------------------
# Logging
# ---------------------------------------------------------------------------
[logging]
level = "INFO"               # Log level for the sievio logger
propagate = false            # Propagate logs to the root logger when true
fmt = "%(asctime)s %(levelname)s %(name)s: %(message)s" # Logging format string
logger_name = "sievio"  # Base logger name used for all sievio logs

# ---------------------------------------------------------------------------
# Metadata stored in run headers and dataset cards
# ---------------------------------------------------------------------------
[metadata]
# Unset by default; runner helpers often fill these from sink paths or GitHub metadata.
# primary_jsonl = "out/data.jsonl"     # Canonical path to the primary JSONL (used in run headers and dataset cards)
# prompt_path = "out/data.prompt.txt"  # Canonical path to the prompt text file
# repo_url = "https://github.com/example/repo" # Source repository URL associated with this run

[metadata.extra]
# Arbitrary key/value metadata to carry through RunSummaryMeta and dataset-card fragments.
# run_label = "example-run"
# notes = "Free-form metadata carried through RunSummaryMeta."

# ---------------------------------------------------------------------------
# Dataset card guidance (Hugging Face style)
# ---------------------------------------------------------------------------
[dataset_card]
enabled = true               # Enable writing dataset-card fragments for each run
split_name = "train"         # Default split name (e.g., "train", "validation", "test")
# license = "MIT"              # SPDX license string or list (used in dataset cards)
# task_categories = "text-classification" # Task category label(s) for the dataset card
# task_ids = "question-answering"         # Task id(s) for the dataset card (e.g., HF task ids)
# tags = ["code", "docs"]      # Additional tags applied to the dataset card
